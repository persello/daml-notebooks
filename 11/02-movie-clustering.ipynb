{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General instructions\n",
    "\n",
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel/runtime** (Colab: in the menubar, select *Runtime*$\\rightarrow$*Factory Reset Runtime*; Jupyter: in the menubar, select *Kernel*$\\rightarrow$*Restart*) and then **run all cells** (Colab: in the menubar, select *Runtime*$\\rightarrow$*Run all*; Jupyter: in the menubar, select *Cell*$\\rightarrow$*Run All*).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or `\"YOUR ANSWER HERE\"`, as well as the list of the group members in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter here the *Group Name* and the list of *Group Members*.\n",
    "\n",
    "`GROUP NAME`\n",
    "\n",
    "`GROUP MEMBERS`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to have an evaluation DO NOT delete/cut the cells with code and answers. Once you have finished you can downolad the notebook (Colab: in the menubar, select *File*$\\rightarrow$*Download .ipynb*; Jupyter: in the menubar, select *File*$\\rightarrow$*Download as*$\\rightarrow$*Notebook (.ipynb)*) and upload as an assignment on the e-learning platform.\n",
    "\n",
    "The following cell will load the Google Drive extension for the current notebook, when the variable `MOUNT` is `True`. This allow you to mount the Google Drive filesystem for file persistence. The mountpoint will be `/content/gdrive`.\n",
    "Furthermore, it will set the `PATH` variable, from now on, so that if you have to refer to external files you could do that by writing:\n",
    "\n",
    "```python\n",
    "os.path.join(PATH, filename)\n",
    "```\n",
    "\n",
    "This will append the filename after the specific PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "MOUNT = False\n",
    "if 'google.colab' in str(get_ipython()) and MOUNT:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    PATH = '/content/gdrive/MyDrive'\n",
    "else:\n",
    "    PATH = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important warning\n",
    "\n",
    "**⚠️ avoid copying, removing or modifying test cells, if you do that your assignment might be graded wrongly ⚠️**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03216ff9a829a4b9b4428f568ad379ca",
     "grade": false,
     "grade_id": "cell-223898e95adc5529",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "# Load the preprocessed file\n",
    "\n",
    "Load the saved `tfidf_matrix` from the file created using the `joblib.load(filename)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1120c3d3b758764a15a0c09d43b679ec",
     "grade": false,
     "grade_id": "cell-a94b33899d8d233a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "The `sklearn.cluster.AgglomerativeClustering` class performs a hierarchical clustering (agglomerative, i.e., bottom-up). The relevant parameters for constructing a clustering model are:\n",
    "\n",
    "* `n_clusters` the number of clusters (that is, where to cut the *dendrogram*)\n",
    "* `affinity` the *similarity* measure to be used ('euclidean', 'manhattan', 'cosine', 'precomputed)\n",
    "* `linkage` the type of linkage to use to decide agglomeration ('ward', 'complete' $\\rightarrow$ maximum, 'average', 'single' $\\rightarrow$ minimum)\n",
    "\n",
    "Try to create an agglomerative clustering model with 5 clusters using the `cosine` similarity and fit the model. Use the `fit_predict()` function that automatically returns the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "846340df835ab3540a83c423fe01c72e",
     "grade": false,
     "grade_id": "cell-63817186b9b4ef72",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Compute the silhouette score of the given clustering. To do that, you can use the `sklearn.metrics.silhouette_score()` function. Notice that you have to specify the `metric` parameter in order to be consistent with the affinity measure used in the clustering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eab0f773716ec19d02f4e8b0d256cc75",
     "grade": false,
     "grade_id": "cell-8a69094345a66f13",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Try to apply the **elbow** method to select the best value of the number of clusters for the `AgglomerativeClustering` (leaving all the other parameters unchanged). In particular, try to get the silhouette score for all the values in the range $[2, 10]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2274d08414d5d9781ff929da41d5ea4",
     "grade": false,
     "grade_id": "cell-a45855e65dbdbfc6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "“Open the clusters” and look at those movies that are finishing in the same cluster, can you spot some similarities? Briefly report your considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5eec1a468ec6d169ba331f25ac104c8",
     "grade": false,
     "grade_id": "cell-ab936c97f2e6a8c3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Given the high dimensionality of the data, it is not possible to plot the data points on a 2D plane. A possible way to overcome this problem is to get a projection of the data using the so-called *Principal Component Analysis*, which is a dimensionality reduction (or deconmposition) technique that tries to summarize the data in a smaller number of meaningful dimensions, which however try to capture the variability of the original data. \n",
    "\n",
    "You can refer to the [sklearn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) of PCA for some detail. In particulawr, try to decompose your data to just 2 dimensions (i.e., components) and plot them by assigning a different color to each different cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f143990445971cd3f6b949e69ede1d22",
     "grade": false,
     "grade_id": "cell-f3f0f4d7712e8259",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Perform the same kind of analysis (i.e., all the steps including also the decision on the number of clusters if applicable) using the `dbscan` and the `kmeans` algorithms. Are there differences in the clusters? Can you visually emphasize them (i.e., by plotting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
